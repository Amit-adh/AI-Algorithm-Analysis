{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMFDcNmwGmAeXRu+VwpmYha"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Bias Analysis of Sentiment Analysis Models and Datasets"],"metadata":{"id":"mZExH7wwdOy8"}},{"cell_type":"markdown","source":["The process of bias analysis is done in 3 steps:\n","\n","\n","1.   First, train and test on a logistic regression model\n","2.   Second, fine tune and test a standard dataset on another model\n","3.   Lastly, train the same model from step 2 on a toxicity dataset\n","\n","Following these 3 steps, we then analyse the bias either inherent in the model or gradually learnt from the training in the provided datasets.\n","\n"],"metadata":{"id":"sBmNvoeHdddv"}},{"cell_type":"code","source":[],"metadata":{"id":"_TDIViWfdTT7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Basic test of Bias using Logistic Regression\n","\n","We first create a baseline model in which we test whether any kind of bias exists in a simple model such as a logistic regression model. This model is trained on the Stanford Sentiment Treebank v2 (SST2) dataset and then tested on the Equity Evaluation Corpus (EEC) dataset.\n","\n","This is then further utilized as a basis for bias analysis in our control model and then our actual testing model"],"metadata":{"id":"SGXTPYycpdKj"}},{"cell_type":"markdown","source":["### For obtaining the datasets from kaggle:"],"metadata":{"id":"z_IjlmcXfhQB"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"atulanandjha/stanford-sentiment-treebank-v2-sst2\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD4aDiK4f1nm","executionInfo":{"status":"ok","timestamp":1762589127826,"user_tz":-330,"elapsed":6037,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"44cc0d63-3337-46a7-8e7c-c4b612783fb3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/atulanandjha/stanford-sentiment-treebank-v2-sst2?dataset_version_number=30...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19.1M/19.1M [00:00<00:00, 168MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/atulanandjha/stanford-sentiment-treebank-v2-sst2/versions/30\n"]}]},{"cell_type":"markdown","source":["### Now the actual definition of the Logistic regression model along with its loss function and optimizer are as follows:"],"metadata":{"id":"h16De8Jlf_Ht"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hmbdpogWMbJr","executionInfo":{"status":"ok","timestamp":1762509013790,"user_tz":-330,"elapsed":11396,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"2290ff1e-6936-4aea-e463-d2c49d366b89"},"outputs":[{"output_type":"stream","name":"stdout","text":["50.06367874145508\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","# configure device to use gpu if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# hyperparameters\n","learning_rate = 1e-3\n","num_epochs = 500\n","\n","class LogisticRegression(nn.Module):\n","  def __init__(self, input_size):\n","    super(LogisticRegression, self).__init__()\n","    self.linear = nn.Linear(input_size, 1)\n","\n","  def forward(self, x):\n","    y_predicted = torch.sigmoid(self.linear(x))\n","    return y_predicted\n","\n","\n","model = LogisticRegression(input_size).to(device)\n","\n","# loss function and optimizer function\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# train the model\n","for epoch in range(num_epochs):\n","  out_data = model(x_train)\n","\n","  l = criterion(out_data, y_train)\n","  l.backward()\n","\n","  optimizer.step()\n","  optimizer.zero_grad()\n","\n","print(model(x_test).item())\n","\n","\n","\n"]}]}