{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOjZuL5NpMu/TNgVsYjLRL5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Bias Analysis of Sentiment Analysis Models and Datasets"],"metadata":{"id":"mZExH7wwdOy8"}},{"cell_type":"markdown","source":["The process of bias analysis is done in 3 steps:\n","\n","\n","1.   First, train and test on a logistic regression model\n","2.   Second, fine tune and test a standard dataset on another model\n","3.   Lastly, train the same model from step 2 on a toxicity dataset\n","\n","Following these 3 steps, we then analyse the bias either inherent in the model or gradually learnt from the training in the provided datasets.\n","\n"],"metadata":{"id":"sBmNvoeHdddv"}},{"cell_type":"markdown","source":["# 1. Basic test of Bias using Logistic Regression\n","\n","We first create a baseline model in which we test whether any kind of bias exists in a simple model such as a logistic regression model. This model is trained on the Stanford Sentiment Treebank v2 (SST2) dataset and then tested on the Equity Evaluation Corpus (EEC) dataset.\n","\n","This is then further utilized as a basis for bias analysis in our control model and then our actual testing model"],"metadata":{"id":"SGXTPYycpdKj"}},{"cell_type":"markdown","source":["### For obtaining the datasets from kaggle:"],"metadata":{"id":"z_IjlmcXfhQB"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"jkhanbk1/sst2-dataset\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD4aDiK4f1nm","executionInfo":{"status":"ok","timestamp":1762592943135,"user_tz":-330,"elapsed":2474,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"57194343-e95d-4e11-e669-57fdd3e3558a"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/jkhanbk1/sst2-dataset?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 827k/827k [00:00<00:00, 114MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n","Path to dataset files: /root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import os\n","# for the csv datasets\n","print(os.listdir('/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cgkHatH4qvwx","executionInfo":{"status":"ok","timestamp":1762593781619,"user_tz":-330,"elapsed":8,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"c9f17977-e3de-44ac-cfcc-f0b31a5899b1"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["['train.csv', 'unsup.csv', 'test.csv', 'val.csv']\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","train_data = '/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format/train.csv'\n","test_data = '/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format/test.csv'\n","val_data = '/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format/val.csv'\n","\n","train_df = pd.read_csv(train_data)\n","test_df = pd.read_csv(test_data)\n","val_df = pd.read_csv(val_data)\n","\n","print(train_df.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k57hxIEpxd3x","executionInfo":{"status":"ok","timestamp":1762594160396,"user_tz":-330,"elapsed":32,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"ff8b87b1-59e6-4cb4-e811-7710d1438b05"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['label', 'sentence'], dtype='object')\n"]}]},{"cell_type":"markdown","source":["### Now the actual definition of the Logistic regression model along with its loss function and optimizer are as follows:"],"metadata":{"id":"h16De8Jlf_Ht"}},{"cell_type":"code","execution_count":37,"metadata":{"id":"hmbdpogWMbJr","executionInfo":{"status":"ok","timestamp":1762595797892,"user_tz":-330,"elapsed":10713,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}}},"outputs":[],"source":["import re, math, hashlib\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Configure device to use gpu if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyper-parameters\n","input_dim = 20000\n","learning_rate = 1e-3\n","num_epochs = 50\n","\n","\n","# Simpler tokenizer and hashing vectorizer\n","token_pat = re.compile(r\"\\w+\")\n","def tokenize(text):\n","  return token_pat.findall(str(text).lower())\n","\n","class HashingVectorizer:\n","  def __init__(self, n_features=20000):\n","    self.n_features = n_features\n","\n","  def _idx(self, token):\n","    return int(hashlib.md5(token.encode(\"utf-8\")).hexdigest(), 16) % self.n_features\n","\n","  def transform_one(self, text):\n","    x = torch.zeros(self.n_features, dtype=torch.float32)\n","    for tok in tokenize(text):\n","        x[self._idx(tok)] += 1.0\n","    n = torch.linalg.norm(x)\n","    if n > 0: x /= n\n","    return x\n","\n","vectorizer = HashingVectorizer(input_dim)\n","\n","\n","# To wrap the pandas dataframe so that torch can read from it better\n","class FrameDataset(Dataset):\n","  def __init__(self, df, text_col='sentence', label_col='label'):\n","    self.text = df[text_col].tolist()\n","    self.labels = df[label_col].tolist()\n","\n","  def __len__(self):\n","    return len(self.text)\n","\n","  def __getitem__(self, i):\n","    return vectorizer.transform_one(self.texts[i]), torch.tensor(self.labels[i], dtype=torch.float32)\n","\n","train_ds = FrameDataset(train_df)\n","val_ds   = FrameDataset(val_df)\n","train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)  # randomizes each epoch\n","val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False) # deterministic epochs\n","\n","\n","class LogisticRegression(nn.Module):\n","  def __init__(self, input_size):\n","    super(LogisticRegression, self).__init__()\n","    self.linear = nn.Linear(input_size, 1)\n","\n","  def forward(self, x):\n","    y_predicted = torch.sigmoid(self.linear(x))\n","    return y_predicted\n","\n","\n","model = LogisticRegression(input_dim).to(device)\n","criterion = nn.BCELoss()  # Loss Function\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"]},{"cell_type":"markdown","source":["### Model Training Loop"],"metadata":{"id":"F0-xyeiR5XFG"}},{"cell_type":"code","source":["# train the model\n","for epoch in range(num_epochs):\n","  out_data = model(x_train)\n","\n","  l = criterion(out_data, y_train)\n","  l.backward()\n","\n","  optimizer.step()\n","  optimizer.zero_grad()\n","\n","print(model(x_test).item())"],"metadata":{"id":"6Wdxl-fx0FOD"},"execution_count":null,"outputs":[]}]}