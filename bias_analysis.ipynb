{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNU3nPG/sTTw3HG8g5o+OmL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Bias Analysis of Sentiment Analysis Models and Datasets"],"metadata":{"id":"mZExH7wwdOy8"}},{"cell_type":"markdown","source":["The process of bias analysis is done in 3 steps:\n","\n","\n","1.   First, train and test on a logistic regression model\n","2.   Second, fine tune and test a standard dataset on another model\n","3.   Lastly, train the same model from step 2 on a toxicity dataset\n","\n","Following these 3 steps, we then analyse the bias either inherent in the model or gradually learnt from the training in the provided datasets.\n","\n"],"metadata":{"id":"sBmNvoeHdddv"}},{"cell_type":"markdown","source":["# 1. Basic test of Bias using Logistic Regression\n","\n","We first create a baseline model in which we test whether any kind of bias exists in a simple model such as a logistic regression model. This model is trained on the Stanford Sentiment Treebank v2 (SST2) dataset and then tested on the Equity Evaluation Corpus (EEC) dataset.\n","\n","This is then further utilized as a basis for bias analysis in our control model and then our actual testing model"],"metadata":{"id":"SGXTPYycpdKj"}},{"cell_type":"markdown","source":["### For obtaining the datasets from kaggle:"],"metadata":{"id":"z_IjlmcXfhQB"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"jkhanbk1/sst2-dataset\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD4aDiK4f1nm","executionInfo":{"status":"ok","timestamp":1762592943135,"user_tz":-330,"elapsed":2474,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"57194343-e95d-4e11-e669-57fdd3e3558a"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/jkhanbk1/sst2-dataset?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 827k/827k [00:00<00:00, 114MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n","Path to dataset files: /root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import os\n","# for the csv datasets\n","print(os.listdir('/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cgkHatH4qvwx","executionInfo":{"status":"ok","timestamp":1762593781619,"user_tz":-330,"elapsed":8,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"c9f17977-e3de-44ac-cfcc-f0b31a5899b1"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["['train.csv', 'unsup.csv', 'test.csv', 'val.csv']\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","train_data = '/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format/train.csv'\n","test_data = '/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format/test.csv'\n","val_data = '/root/.cache/kagglehub/datasets/jkhanbk1/sst2-dataset/versions/1/Finalv SST-2 dataset CSV format/val.csv'\n","\n","train_df = pd.read_csv(train_data)\n","test_df = pd.read_csv(test_data)\n","val_df = pd.read_csv(val_data)\n","\n","print(train_df.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k57hxIEpxd3x","executionInfo":{"status":"ok","timestamp":1762594160396,"user_tz":-330,"elapsed":32,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"ff8b87b1-59e6-4cb4-e811-7710d1438b05"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['label', 'sentence'], dtype='object')\n"]}]},{"cell_type":"markdown","source":["### Now the actual definition of the Logistic regression model along with its loss function and optimizer are as follows:"],"metadata":{"id":"h16De8Jlf_Ht"}},{"cell_type":"code","execution_count":56,"metadata":{"id":"hmbdpogWMbJr","executionInfo":{"status":"ok","timestamp":1762597909717,"user_tz":-330,"elapsed":14,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}}},"outputs":[],"source":["import re, math, hashlib\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Configure device to use gpu if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyper-parameters\n","input_dim = 20000\n","learning_rate = 1e-3\n","num_epochs = 25\n","\n","\n","# Simpler tokenizer and hashing vectorizer\n","token_pat = re.compile(r\"\\w+\")\n","def tokenize(text):\n","  return token_pat.findall(str(text).lower())\n","\n","class HashingVectorizer:\n","  def __init__(self, n_features=20000):\n","    self.n_features = n_features\n","\n","  def _idx(self, token):\n","    return int(hashlib.md5(token.encode(\"utf-8\")).hexdigest(), 16) % self.n_features\n","\n","  def transform_one(self, text):\n","    x = torch.zeros(self.n_features, dtype=torch.float32)\n","    for tok in tokenize(text):\n","        x[self._idx(tok)] += 1.0\n","    n = torch.linalg.norm(x)\n","    if n > 0: x /= n\n","    return x\n","\n","vectorizer = HashingVectorizer(input_dim)\n","\n","\n","# To wrap the pandas dataframe so that torch can read from it better\n","class FrameDataset(Dataset):\n","  def __init__(self, df, text_col='sentence', label_col='label'):\n","    self.texts = df[text_col].tolist()\n","    self.labels = df[label_col].tolist()\n","\n","  def __len__(self):\n","    return len(self.texts)\n","\n","  def __getitem__(self, i):\n","    return vectorizer.transform_one(self.texts[i]), torch.tensor([self.labels[i]], dtype=torch.float32)\n","\n","train_ds = FrameDataset(train_df)\n","test_ds   = FrameDataset(test_df)\n","train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)  # randomizes each epoch\n","test_loader   = DataLoader(test_ds,   batch_size=256, shuffle=False) # deterministic epochs\n","\n","\n","class LogisticRegression(nn.Module):\n","  def __init__(self, input_size):\n","    super(LogisticRegression, self).__init__()\n","    self.linear = nn.Linear(input_size, 1)\n","\n","  def forward(self, x):\n","    y_predicted = torch.sigmoid(self.linear(x))\n","    return y_predicted\n","\n","\n","model = LogisticRegression(input_dim).to(device)\n","criterion = nn.BCELoss()  # Loss Function\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"]},{"cell_type":"markdown","source":["### Model Training Loop"],"metadata":{"id":"F0-xyeiR5XFG"}},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","  # Train the model\n","  model.train()\n","  total_loss = 0.0\n","  for xb, yb in train_loader:\n","    xb, yb = xb.to(device), yb.to(device)\n","    optimizer.zero_grad()\n","    out_logits = model(xb)\n","    loss = criterion(out_logits, yb)\n","    loss.backward()\n","    optimizer.step()\n","    total_loss += loss.item() * xb.size(0)\n","  avg_loss = total_loss / len(train_loader.dataset)\n","\n","# Evaluate the model in the current epoch\n","  model.eval()\n","  correct, total = 0, 0\n","  with torch.no_grad():\n","    for xb, yb in test_loader:\n","      xb, yb = xb.to(device), yb.to(device)\n","      preds = (torch.sigmoid(model(xb)) >= 0.5).long()\n","      correct += (preds == yb.long()).sum().item()\n","      total += yb.size(0) # no of elements in the tensor\n","    acc = correct / total\n","    print(f\"Epoch {epoch}: train_loss={avg_loss:.4f}  val_acc={acc:.4f}\")\n","\n","\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Wdxl-fx0FOD","executionInfo":{"status":"ok","timestamp":1762597990698,"user_tz":-330,"elapsed":75312,"user":{"displayName":"Amit Adhikari","userId":"04710891950475841681"}},"outputId":"2f310021-9d5d-4c47-f3bc-eee51c571d26"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: train_loss=0.6901  val_acc=0.4992\n","Epoch 1: train_loss=0.6820  val_acc=0.4992\n","Epoch 2: train_loss=0.6745  val_acc=0.4992\n","Epoch 3: train_loss=0.6673  val_acc=0.4992\n","Epoch 4: train_loss=0.6605  val_acc=0.4992\n","Epoch 5: train_loss=0.6539  val_acc=0.4992\n","Epoch 6: train_loss=0.6474  val_acc=0.4992\n","Epoch 7: train_loss=0.6411  val_acc=0.4992\n","Epoch 8: train_loss=0.6349  val_acc=0.4992\n","Epoch 9: train_loss=0.6290  val_acc=0.4992\n","Epoch 10: train_loss=0.6232  val_acc=0.4992\n","Epoch 11: train_loss=0.6175  val_acc=0.4992\n","Epoch 12: train_loss=0.6119  val_acc=0.4992\n","Epoch 13: train_loss=0.6065  val_acc=0.4992\n","Epoch 14: train_loss=0.6012  val_acc=0.4992\n","Epoch 15: train_loss=0.5960  val_acc=0.4992\n","Epoch 16: train_loss=0.5910  val_acc=0.4992\n","Epoch 17: train_loss=0.5860  val_acc=0.4992\n","Epoch 18: train_loss=0.5812  val_acc=0.4992\n","Epoch 19: train_loss=0.5765  val_acc=0.4992\n","Epoch 20: train_loss=0.5719  val_acc=0.4992\n","Epoch 21: train_loss=0.5673  val_acc=0.4992\n","Epoch 22: train_loss=0.5629  val_acc=0.4992\n","Epoch 23: train_loss=0.5586  val_acc=0.4992\n","Epoch 24: train_loss=0.5542  val_acc=0.4992\n"]}]},{"cell_type":"code","source":["def predict(sentence: str):\n","    x = vectorizer.transform_one(sentence).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        p = torch.sigmoid(model(x)).item()\n","    return p, (\"positive\" if p >= 0.5 else \"negative\")\n"],"metadata":{"id":"8WV5-Gzd-Mcf"},"execution_count":null,"outputs":[]}]}